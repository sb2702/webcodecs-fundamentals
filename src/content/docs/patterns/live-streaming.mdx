---
title: Live Streaming with WebCodecs
description: Low-latency encoder pipelines
---

import { Tabs, TabItem } from '@astrojs/starlight/components';


There are many ways to stream video over the internet, and of all the ways to deal with video streaming in the browser (`<video>`, MSE, WebRTC), WebCodecs provides the lowest level control. 

Emerging technologies like [Media Over Quic](../../projects/moq.md) use WebCodecs to provide lower latency and higher scalability than with previous options.

Unlike a transcoding pipeline, where it's pretty clear cut what needs to be done, how you would build a video-streaming application depends entirely on what you are trying to do.

Here are just a few kinds of applications with some form of video streaming:
* An application to watch live sports broadcasts
* Recording software to stream your webcam to multiple social-media live-streaming platforms
* A webinar tool where a few hosts stream content to many attendees

Each would have a different architecture, and for each there might be multiple ways to accomplish the same thing, so the choice of WebCodecs vs MSE vs WebRTC etc.. becomes a design choice.

 To make this manageable, I'll focus on how to do the following with WebCodecs:

* Stream video from a browser to a browser (e.g. video conferencing)
* Stream video from a browser to a server (e.g. recording studio)
* Stream video from a server to a browser (e.g. live broadcast)

I'll then provide a quick overview of the alternatives (WebRTC, MSE) and include some real world case studies of streaming applications and their architectures, to help you decide if and where WebCodecs makes sense.

Because WebCodecs works with binary encoded video data, it's a lot easier to integrate with server media processing libraries like ffmpeg and gstreamer. 
I'll therefore assume you can do whatever prcessing you need based on your application's business logic with server media processing libraries, and I'll stick to how you'd stream video to/from the browser with WebCodecs.

## Data transfer

Raw video is too large to stream over a network, so in any streaming scenario we'll be working with encoded video and audio data. For WebCodecs specifically, we'll primarily be working with `EncodedVideoChunk` and `EncodedAudioChunk` objects.

Other browser APIs like `WebRTC`and `MSE` have data transfer built-in and application developers don't normally manage how individual video packets are sent over the network.

The strength and weakness of WebCodecs is that it is very low-level, so you absolutely can control how individual video packets are sent over the network, but then you have to figure out how to send invidual video packets over the network.


#### The requirements

Let's start with the example of sending a video stream from the browser to a server. Our `VideoEncoder` gives us `EncodedVideoChunk` objects, and an `EncodedVideoChunk` is just binary data with some meta data attached.

![](/assets/patterns/livestreaming/encoded-chunk-2.svg)

For a streaming, we'd need to send a bunch of these chunks, in real time, in order, over a network, in a bidirectional manner.

![](/assets/patterns/livestreaming/encoded-chunk-3.svg)


With generic data transfer, there are a number of ways we could accomplish this:


#### The do-it-yourself networking options

###### HTTP

You could theoretically expose an HTTP endpoint on your server and `POST` every frame as data. Here's a simplified example where metadata is sent as a header

<Tabs>

  <TabItem label="Client">


  ```typescript
const buffer = new Uint8Array(chunk.byteLength)
chunk.copyTo(buffer);

fetch('/upload', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/octet-stream',
      'X-Timestamp': chunk.timestamp.toString(),
      'X-Index': chunkIndex.toString(),
      'X-Keyframe': chunk.type === 'key' ? '1' : '0',
    },
    body: chunk.data,
});

```
  
  </TabItem>

  <TabItem label="Server">
```typescript
app.post('/upload', (req, res) => {
    const timestamp = parseInt(req.headers['x-timestamp']);
    const keyframe = req.headers['x-keyframe'] === '1';
    const chunkIndex = parseInt(req.headers['x-index']);
    const data = req.body;
    res.json({ ok: true });
});
```
  </TabItem>
</Tabs>

For streaming this is impractical, as you'd be sending dozens of http requests per second which is slow and error prone. There is also no way to send data back to the client. You could make http work if you just need to upload media in chunks without worrying about real-time, but there are also inherently more suitable options.


###### Web Sockets

Web Sockets enables you create a persistent connection with a server, and it enables bidirectional flows of data. Here you could come up with your own binary encoding scheme to fit metadata and chunk data together into a single binary buffer and send those between server and client.

<Tabs>


  <TabItem label="Client">


  ```typescript
const ws = new WebSocket('ws://localhost:3000/upload');

const encoder = new VideoEncoder({
    output: (chunk) => {
        const binaryData = <Uint8Array> customBinaryEncodingScheme(chunk);
        ws.send(binaryData);
    },
    error: (e)=>{} //error handling
 });


```
  
  </TabItem>

  <TabItem label="Custom Binary Scheme">
  ```typescript
// Just an example, this is not official or canonical but it would work
function customBinaryEncodingScheme(chunk: EncodedVideoChunk): Uint8Array {

      const metadata = {
        timestamp: chunk.timestamp,
        keyframe: chunk.type === 'key',
        size: chunk.data.byteLength,
      };
      
      // Format: [metadata JSON length (4 bytes)] [metadata JSON] [binary data]
      const metadataStr = JSON.stringify(metadata);
      const metadataBytes = new TextEncoder().encode(metadataStr);
      
      const frame = new ArrayBuffer(4 + metadataBytes.length + chunk.data.byteLength);
      const view = new DataView(frame);
      
      // Write metadata length as 32-bit integer
      view.setUint32(0, metadataBytes.length, true);
      
      // Write metadata
      new Uint8Array(frame, 4, metadataBytes.length).set(metadataBytes);
      
      // Write binary data
      return new Uint8Array(frame, 4 + metadataBytes.length).set(new Uint8Array(chunk.data)
};
```
  </TabItem>

  <TabItem label="Server">
```typescript
const express = require('express');
const WebSocket = require('ws');
const app = express();
const server = http.createServer(app);
const wss = new WebSocket.Server({ server });

wss.on('connection', (ws) => {
 
  ws.on('message', (data) => {
       // data is a Buffer
       const chunk = parseCustomBinary(data);
  });

});


```
  </TabItem>
</Tabs>

If you are only sending data one way, this could work, but if you need to enable data from one browser to another, you'd need your server to concurrently handle multiple websocket sessions and set up your own routing system.

###### Web Transport

WebTransport is like a successor to WebSockets, with support being rolled out [[2](https://caniuse.com/webtransport)],  but it enables better peformance and uses the [Streams API](../../concepts/streams). WebTransport lets you write this as a pipeline, where the video frame source (e.g. a user webcam) gets piped through the encooder (a `TransformStream` wrapper) and piped to a writer, which then writes data in a stream-like fashion over the network.

 ```typescript
const transport = await WebTransport.connect('https://localhost:3000/upload');
const stream = await transport.createUnidirectionalStream();
const writer = stream.getWriter();

 //PseudoCode
frameSource.pipeThrough(videoEncoder).pipeTo(writer);
 ```

This is a practical option for unidirectional streams, though like WebSockets, if you need to enable data from one browser to another, youâ€™d need to set up your own routing system.

#### Media over Quic

[Media over Quic](../../projects/moq) is a new protocol specifically designed for this use case of facilitating delivery of WebCodecs and other low-level streaming data without the need for 'do-it-yourself' networking.

It works as a [pub-sub](https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern) system, where a *publisher* can publish a stream of data to a *relay*, and a *subscriber* can subscribe to streams of data from a *relay*.

![](/assets/patterns/livestreaming/media-over-quic.svg)

Unlike WebRTC servers (which are also relays) Media over Quic relays are content-agnostic and don't rely on 'session state', making it more scalable. You can self host a relay [[2](https://github.com/moq-dev/moq)], but several CDN providers also offer Media Over Quick relays [[3](https://blog.cloudflare.com/moq/)].




<Tabs>


  <TabItem label="Publisher">


  ```typescript
import * as Moq from "@moq/lite";

const connection = await Moq.connect("https://relay.moq.some-cdn.com");

const broadcast = new Moq.Broadcast();

connection.publish('my-broadcast', broadcast);

// Pull-mode, tracks are created when subscribers subscribe
const {track, priority} =  await broadcast.requested();

if(track.name ==='chat'){
    const group = track.appendGroup();
    group.writeString("Hello, MoQ!");
    group.close();
}


```
  
  </TabItem>

  <TabItem label="Subscriber">

```typescript

import * as Moq from "@moq/lite";

const connection = await Moq.connect("https://relay.moq.some-cdn.com");

// Subscribe to a broadcast
const broadcast = connection.consume("my-broadcast");

// Subscribe to a specific track
const track = await broadcast.subscribe("chat");

// Read data as it arrives
for (;;) {
	const group = await track.nextGroup();
	if (!group) break;

	for (;;) {
		const frame = await group.readString();
		if (!frame) break;
        console.log("Received:", frame);
    }
}
```
  </TabItem>
</Tabs>


Here's a quick demo of MoQ using Clouldflare's public relay, where one iframe will publish text messages in a track, and the other a iframe will subscribe to the same track, and listen for messages from the relay


<iframe src="/demo/moq/index2.html" style="width: 100%; height: 650px; border: 1px solid #e2e8f0; border-radius: 8px;"></iframe>



Media over Quic greatly simplifies the networking aspect (you don't even need to host your own server) while also being performant (CDN relays scale better than a WebRTC server) and providing low-level control over how and when you send encoded video chunks.

This all makes it ideal for our use case of streaming encoded chunks, so that's what we'll use in the rest of our examples.  You are of course free to use any data transfer mechanism you see fit, that is one of the benefits of WebCodecs.



You can find out more about Media over Quic [here](../../projects/moq), it's worth a read in general but for now it's time to get to code.



## Getting webcam data

1) MediaDevices getUserMedia

2) VideoEncoder (transform stream)

3) Instorage Memory

## Examples

### Browser to Browser

#### Hang format
##listen to track.json


-- Production gotchas




### Server to browser

-- Production gotchas





### Browser to Server

-- parsing the data locally

-- Saving it locally

-- Production gotchas


## Alternatives

#### WebRTC

#### Media Source Extensions


## Case Studies

Now that you've seen how WebCodecs works and how to stream, let's look at some concrete examples of actual streaming applications and their architectures: [Session](https://www.youtube.com/watch?v=WispwTzPS9A) (a webinar tool, discontinued), and [Streamyard](https://streamyard.com/) (a browser based recording studio). I worked on both products in a previous role.

#### Session (Webinars)

Session was a webinar product which enabled several hosts to conduct video webinars with up to 5000 participants. It had common interactive webinar features like polls, Q&A and breakout rooms.

![](/assets/patterns/livestreaming/session2.jpeg)

Session was built with [WebRTC](https://webrtc.org/), which is an WebAPI that is specifically designed to facilitate video conferencing, and is what is used in browser based video conference tools like Google Meet.

Session used a router/relay model, in which every participant streamed audio/video a routing server, and then a server (specifically an [SFU](https://bloggeek.me/webrtcglossary/sfu/)) would then apply it's business logic to route some subset of streams to each participant without re-encoding.

![](/assets/patterns/livestreaming/session-stack.svg)

In the webinar use case, the hosts would broadcast to all participants (few to many), but each participant would only see a subset of other participants, and this subset could change based on factors like joining a breakout room, and participants joining and leaving.

WebRTC was probably the best solution in this case, as it facilitates coordination of codec choice among all participants, abstracts the encoding details and is optimized for real-time delivery. However, there were still some challenges with using WebRTC for Session:

**Scale**: WebRTC starts facing scalability issues beyond 100 participants [[1](https://bloggeek.me/how-many-users-webrtc-call/)]. This was fine for most webinars, but a few high-value webinars attracted thousands of participants, and Session was only able to scale to 5000 participants through in-house WebRTC expertise.

**Video Quality**: WebRTC provided limited control over video quality, and by necessity provided lower quality to ensure low latency. This became relevant when enabling recording of webinars (a common use case), where the recording was essentially a server joining the webinar as a participant, and recording the compressed video feeds in real time, resulting in *okay* quality recordings, but not matching the level of locally recorded content.

WebCodecs via Media over Quic could likely have solved the scale issue, handling many more concurrent subscribers than is possible with WebRTC. Media over Quic is particularly well suited to this "middle-ground" for webinars where there are too many participants for a normal video conferencing call (for which WebRTC was designed), but not enough scale for full-fledged HLS/DASH streaming with millions of viewers.


However as a newer protocol with fewer supporting libraries, and more decision making required on codec configuration and delivery, it would have required more up-front engineering and experimentation to work. 


#### Streamyard

Streamyard is a browser based recording studio, that enables participants to live stream their webcam feeds to multiple social media destinations (like Facebook Live, YouTube live) simultaneously.

![](/assets/patterns/livestreaming/streamyard.png)

Streamyard had a more complicated setup where participants streamed their webcam feeds via WebRTC through a routing server. Functionally, it operated like a WebRTC conferencing call, but there were also recording servers listening to each "call" as another WebRTC participant (running a version of the app on the server via headless browsers), recording the feeds from each participant and compositing them into the layout the host specified in the studio.

![](/assets/patterns/livestreaming/streamyard-stack.svg)

This composited feed would then be encoded into an RTMP live stream that would then be simultaneously distributed to multiple destinations (e.g. Facebook Like, YouTube live etc...). Streamyard was built before WebCodecs was released, so it wasn't an option when the product first launched.

**Video Quality**: Many live streamers wanted higher quality versions of their recordings which they could edit after the live stream. Because the WebRTC feeds provided lower video quality (to ensure latency), Streamyard implemented a system to record higher-quality versions of each stream within the browser (called "local crecordings"), and send that to the server in chunks, in parallel to the WebRTC stream. This meant that in practice, the browser was recording two video streams in parallel.


**Complexity**: Because feeds were being converted to RTMP feeds and sent to livestreaming destinations with their own CDNs, Streamyard never faced a scale issue, however the mixing of numerous technologies added complexity and failure points, and it worked at a business level only because of lots of engineering and a deep internal focus on stability.


While working with RTMP was a core requirement (that's what live-streaming destinations required as inputs), Streamyard had multiple options for sending video from the browser to the server, and in practice ended up using two parallel systems (WebRTC for the livestream, an in-house solution for local recordings).

If you were to build a product like Streamyard today, given the product's core focus on stability, using the established WebRTC protocol over a newer, more experimental setup with WebCodecs probably makes sense. That said, I do know that the Streamyard team specifically preferred controlling certain aspects of their stack to obtain more reliability and performance compared to off-the-shelf solutions, and so having more fine-grained control over encoding and networking compared to WebRTC might would have also made it a sensible choice.

For the local recordings use case (recording higher-quality versions of the stream locally, uploading that in chunks), that seems like a pretty good use case for WebCodecs.


#### Depends on your use case

Hopefully you can see from the above-two examples that how you architect a streaming application depends on what you are doing, you may have multiple options for streaming media, each option has it's strengths and weaknesses, and business requirements would often require custom logic beyond what any standard technology can provide by itself.

Maybe WebCodecs doesn't make sense for your application, or maybe it's a no brainer, or maybe it might make sense for certain features.  Whatever the case, hopefully this article gave you an idea of how WebCodecs can be used in streaming contexts, and provided enough info to make a more informed decision on how to architect a streaming application.