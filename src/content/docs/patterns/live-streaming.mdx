---
title: Streaming video with WebCodecs
description: How to live stream with WebCodecs and Media over Quic (MoQ)
---

import { Tabs, TabItem } from '@astrojs/starlight/components';


There are many ways to stream video over the internet, and of all the ways to deal with video streaming in the browser (`<video>`, MSE, WebRTC), WebCodecs provides the lowest level control. 

Emerging technologies like [Media Over Quic](../../projects/moq.md) use WebCodecs to provide lower latency and higher scalability than with previous options.

Unlike a transcoding pipeline, where it's pretty clear cut what needs to be done, how you would build a video-streaming application depends entirely on what you are trying to do.

Here are just a few kinds of applications with some form of video streaming:
* An application to watch live sports broadcasts
* Recording software to stream your webcam to multiple social-media live-streaming platforms
* A webinar tool where a few hosts stream content to many attendees

Each would have a different architecture, and for each there might be multiple ways to accomplish the same thing, so the choice of WebCodecs vs MSE vs WebRTC etc.. becomes a design choice.

 To make this manageable, I'll focus on how to do the following with WebCodecs:

* Stream video from a browser to a browser (e.g. video conferencing)
* Stream video from a browser to a server (e.g. recording studio)
* Stream video from a server to a browser (e.g. live broadcast)

I'll then provide a quick overview of the alternatives (WebRTC, MSE) and include some real world case studies of streaming applications and their architectures, to help you decide if and where WebCodecs makes sense.

Because WebCodecs works with binary encoded video data, it's a lot easier to integrate with server media processing libraries like ffmpeg and gstreamer. 
I'll therefore assume you can do whatever prcessing you need based on your application's business logic with server media processing libraries, and I'll stick to how you'd stream video to/from the browser with WebCodecs.

## Data transfer

Raw video is too large to stream over a network, so in any streaming scenario we'll be working with encoded video and audio data. For WebCodecs specifically, we'll primarily be working with `EncodedVideoChunk` and `EncodedAudioChunk` objects.

Other browser APIs like `WebRTC`and `MSE` have data transfer built-in and application developers don't normally manage how individual video packets are sent over the network.

The strength and weakness of WebCodecs is that it is very low-level, so you absolutely can control how individual video packets are sent over the network, but then you have to figure out how to send invidual video packets over the network.


#### The requirements

Let's start with the example of sending a video stream from the browser to a server. Our `VideoEncoder` gives us `EncodedVideoChunk` objects, and an `EncodedVideoChunk` is just binary data with some meta data attached.

![](/assets/patterns/livestreaming/encoded-chunk-2.svg)

For a streaming, we'd need to send a bunch of these chunks, in real time, in order, over a network, in a bidirectional manner.

![](/assets/patterns/livestreaming/encoded-chunk-3.svg)


With generic data transfer, there are a number of ways we could accomplish this:


#### The do-it-yourself networking options

###### HTTP

You could theoretically expose an HTTP endpoint on your server and `POST` every frame as data. Here's a simplified example where metadata is sent as a header

<Tabs>

  <TabItem label="Client">


  ```typescript
const buffer = new Uint8Array(chunk.byteLength)
chunk.copyTo(buffer);

fetch('/upload', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/octet-stream',
      'X-Timestamp': chunk.timestamp.toString(),
      'X-Index': chunkIndex.toString(),
      'X-Keyframe': chunk.type === 'key' ? '1' : '0',
    },
    body: chunk.data,
});

```
  
  </TabItem>

  <TabItem label="Server">
```typescript
app.post('/upload', (req, res) => {
    const timestamp = parseInt(req.headers['x-timestamp']);
    const keyframe = req.headers['x-keyframe'] === '1';
    const chunkIndex = parseInt(req.headers['x-index']);
    const data = req.body;
    res.json({ ok: true });
});
```
  </TabItem>
</Tabs>

For streaming this is impractical, as you'd be sending dozens of http requests per second which is slow and error prone. There is also no way to send data back to the client. You could make http work if you just need to upload media in chunks without worrying about real-time, but there are also inherently more suitable options.


###### Web Sockets

Web Sockets enables you create a persistent connection with a server, and it enables bidirectional flows of data. Here you could come up with your own binary encoding scheme to fit metadata and chunk data together into a single binary buffer and send those between server and client.

<Tabs>


  <TabItem label="Client">


  ```typescript
const ws = new WebSocket('ws://localhost:3000/upload');

const encoder = new VideoEncoder({
    output: (chunk) => {
        const binaryData = <Uint8Array> customBinaryEncodingScheme(chunk);
        ws.send(binaryData);
    },
    error: (e)=>{} //error handling
 });


```
  
  </TabItem>

  <TabItem label="Custom Binary Scheme">
  ```typescript
// Just an example, this is not official or canonical but it would work
function customBinaryEncodingScheme(chunk: EncodedVideoChunk): Uint8Array {

      const metadata = {
        timestamp: chunk.timestamp,
        keyframe: chunk.type === 'key',
        size: chunk.data.byteLength,
      };
      
      // Format: [metadata JSON length (4 bytes)] [metadata JSON] [binary data]
      const metadataStr = JSON.stringify(metadata);
      const metadataBytes = new TextEncoder().encode(metadataStr);
      
      const frame = new ArrayBuffer(4 + metadataBytes.length + chunk.data.byteLength);
      const view = new DataView(frame);
      
      // Write metadata length as 32-bit integer
      view.setUint32(0, metadataBytes.length, true);
      
      // Write metadata
      new Uint8Array(frame, 4, metadataBytes.length).set(metadataBytes);
      
      // Write binary data
      return new Uint8Array(frame, 4 + metadataBytes.length).set(new Uint8Array(chunk.data)
};
```
  </TabItem>

  <TabItem label="Server">
```typescript
const express = require('express');
const WebSocket = require('ws');
const app = express();
const server = http.createServer(app);
const wss = new WebSocket.Server({ server });

wss.on('connection', (ws) => {
 
  ws.on('message', (data) => {
       // data is a Buffer
       const chunk = parseCustomBinary(data);
  });

});


```
  </TabItem>
</Tabs>

If you are only sending data one way, this could work, but if you need to enable data from one browser to another, you'd need your server to concurrently handle multiple websocket sessions and set up your own routing system.

###### Web Transport

WebTransport is like a successor to WebSockets, with support being rolled out [[2](https://caniuse.com/webtransport)],  but it enables better peformance and uses the [Streams API](../../concepts/streams). WebTransport lets you write this as a pipeline, where the video frame source (e.g. a user webcam) gets piped through the encooder (a `TransformStream` wrapper) and piped to a writer, which then writes data in a stream-like fashion over the network.

 ```typescript
const transport = await WebTransport.connect('https://localhost:3000/upload');
const stream = await transport.createUnidirectionalStream();
const writer = stream.getWriter();

 //PseudoCode
frameSource.pipeThrough(videoEncoder).pipeTo(writer);
 ```

This is a practical option for unidirectional streams, though like WebSockets, if you need to enable data from one browser to another, youâ€™d need to set up your own routing system.

#### Media over Quic

[Media over Quic](../../projects/moq) is a new protocol specifically designed for this use case of facilitating delivery of WebCodecs and other low-level streaming data without the need for 'do-it-yourself' networking.

It works as a [pub-sub](https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern) system, where a *publisher* can publish a stream of data to a *relay*, and a *subscriber* can subscribe to streams of data from a *relay*.

![](/assets/patterns/livestreaming/media-over-quic.svg)

Unlike WebRTC servers (which are also relays) Media over Quic relays are content-agnostic and don't rely on 'session state', making it more scalable. You can self host a relay [[2](https://github.com/moq-dev/moq)], but several CDN providers also offer Media Over Quick relays [[3](https://blog.cloudflare.com/moq/)].




<Tabs>


  <TabItem label="Publisher">


  ```typescript
import * as Moq from "@moq/lite";

const connection = await Moq.connect("https://relay.moq.some-cdn.com");

const broadcast = new Moq.Broadcast();

connection.publish('my-broadcast', broadcast);

// Pull-mode, tracks are created when subscribers subscribe
const {track, priority} =  await broadcast.requested();

if(track.name ==='chat'){
    const group = track.appendGroup();
    group.writeString("Hello, MoQ!");
    group.close();
}


```
  
  </TabItem>

  <TabItem label="Subscriber">

```typescript

import * as Moq from "@moq/lite";

const connection = await Moq.connect("https://relay.moq.some-cdn.com");

// Subscribe to a broadcast
const broadcast = connection.consume("my-broadcast");

// Subscribe to a specific track
const track = await broadcast.subscribe("chat");

// Read data as it arrives
for (;;) {
	const group = await track.nextGroup();
	if (!group) break;

	for (;;) {
		const frame = await group.readString();
		if (!frame) break;
        console.log("Received:", frame);
    }
}
```
  </TabItem>
</Tabs>


Here's a quick demo of MoQ using Clouldflare's public relay, where one iframe will publish text messages in a track, and the other a iframe will subscribe to the same track, and listen for messages from the relay


<iframe src="/demo/moq/index.html" style="width: 100%; height: 650px; border: 1px solid #e2e8f0; border-radius: 8px;"></iframe>



Media over Quic greatly simplifies the networking aspect (you don't even need to host your own server) while also being performant (CDN relays scale better than a WebRTC server) and providing low-level control over how and when you send encoded video chunks.

This all makes it ideal for our use case of streaming encoded chunks, so that's what we'll use in the rest of our examples.  You are of course free to use any data transfer mechanism you see fit, that is one of the benefits of WebCodecs.



You can find out more about Media over Quic [here](../../projects/moq), it's worth a read in general but for now it's time to get to code.



## Getting webcam data

We're going to start with streaming video and audio from the browser. Presumably the most common use case is to stream webcam audio and video and so that's what we're going to start with.


#### getUserMedia

Our first step is to call [getUserMedia](https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia) which is an async call which will request the user's webcam, specifically the audio you request

```typescript
const stream = <MediaStream> await navigator.mediaDevices.getUserMedia({
  video: { width: 1280, height: 720 },
  audio: true,
});
```

This returns a `MediaStream` object, which allows us to both preview the video, and is necessary for reading raw video frames or audio samples, or with use in other APIs like WebRTC.

We then extract the tracks:

```typescript
const videoTrack = <MediaStreamTrack> stream.getVideoTracks()[0];
const audioTrack = <MediaStreamTrack> stream.getAudioTracks()[0];
```

And this lets get the specific track for audio and video.

![](/assets/patterns/livestreaming/mst.svg)

#### MediaStreamTrackProcessor

To actually get raw audio and video data from the stream, we'll need a [MediaStreamTrackProcessor](https://developer.mozilla.org/en-US/docs/Web/API/MediaStreamTrackProcessor). It's not yet supported on Safari or Firefox, but we can use a [polyfill](https://github.com/sb2702/webcodecs-utils/blob/main/src/polyfills/media-stream-track-processor.ts):

```typescript
import { MediaStreamTrackProcessor } from 'webcodecs-utils'  
const videoProcessor = new MediaStreamTrackProcessor({ track: videoTrack });
```

We can then start reading `VideoFrame` objects:

```typescript
const reader = videoProcessor.readable.getReader();

while (true) {
  const { done, value: frame } = await reader.read();
  if (done) break;

  // Process the VideoFrame
  console.log('Frame timestamp:', frame.timestamp);

  // Don't forget to close frames when done!
  frame.close();
}
```

Each `VideoFrame` contains the raw video data along with metadata like timestamp. We need to close frames after processing to free memory.

```typescript
const audioProcessor = new MediaStreamTrackProcessor({ track: audioTrack });
const reader = audioProcessor.readable.getReader();

while (true) {
  const { done, value: audioData } = await reader.read();
  if (done) break;

  // Process the AudioData
  console.log('Audio sample rate:', audioData.sampleRate);
  console.log('Number of frames:', audioData.numberOfFrames);

  // Don't forget to close audio data when done!
  audioData.close();
}
```

The audio processor returns `AudioData` objects containing raw audio samples along with metadata like sample rate and channel count.


![](/assets/patterns/livestreaming/mst-2.svg)

#### Encoding Pipeline

Rather than manually reading frames and encoding them one-by-one, we can use the [Streams API](https://developer.mozilla.org/en-US/docs/Web/API/Streams_API) to create a pipeline. We can wrap `VideoEncoder` and `AudioEncoder` in `TransformStream` objects.

Here's what a `VideoEncoderStream` looks like:

```typescript
class VideoEncoderStream extends TransformStream<VideoFrame, { chunk: EncodedVideoChunk; meta: EncodedVideoChunkMetadata }> {
  constructor(config: VideoEncoderConfig) {
    let encoder: VideoEncoder;
    let frameIndex = 0;

    super({
      start(controller) {
        encoder = new VideoEncoder({
          output: (chunk, meta) => {
            controller.enqueue({ chunk, meta });
          },
          error: (e) => controller.error(e),
        });
        encoder.configure(config);
      },

      async transform(frame, controller) {
        // Apply backpressure
        while (encoder.encodeQueueSize >= 20) {
          await new Promise((r) => setTimeout(r, 10));
        }
        encoder.encode(frame, { keyFrame: frameIndex % 60 === 0 });
        frame.close();
        frameIndex++;
      },

      async flush() {
        await encoder.flush();
        if (encoder.state !== 'closed') encoder.close();
      },
    }, { highWaterMark: 10 });
  }
}
```

The `TransformStream` takes `VideoFrame` as input and outputs `EncodedVideoChunk`. The `start` method sets up the encoder, `transform` encodes each frame, and `flush` cleans up when the stream ends.


```typescript
import { getBitrate, getCodecString } from 'webcodecs-utils';

const videoSettings = videoTrack.getSettings();
const width = videoSettings.width;
const height = videoSettings.height;
const fps = videoSettings.frameRate || 30;

// Calculate optimal bitrate and codec string
const bitrate = getBitrate(width, height, fps, 'good');
const config: VideoEncoderConfig = {
  codec: getCodecString('avc', width, height, bitrate),
  width,
  height,
  bitrate,
  framerate: fps,
};

const videoEncoderStream = new VideoEncoderStream(config);

// Pipe frames through encoder
const encodedVideoStream = videoProcessor.readable
  .pipeThrough(videoEncoderStream);

```

The pipeline will now output `EncodedVideoChunk` objects representing the user's webcam (you can do the same for audio). Using the [Streams API](../../concepts/streams) helps us manage memory and automatically handles setup, flushing and clean up.



![](/assets/patterns/livestreaming/mst-3.svg)


#### Recording WebCam demo

Now we have a stream of `EncodedVideoChunk` and `EncodedAudioChunk` data from our webcam. In the next section, we'll pipe this to the internet, but just to have an intermediate working demo, we'll pipe this data to a file.


First, we'll create a muxer

```typescript
import { Muxer, StreamTarget } from 'mp4-muxer';

const storage = new InMemoryStorage();
const target = new StreamTarget({
  onData: (data: Uint8Array, position: number) => {
    storage.write(data, position);
  },
  chunked: true,
  chunkSize: 1024 * 1024 * 10
});

const muxer = new Muxer({
  target,
  video: {
    codec: 'avc',
    width: videoSettings.width!,
    height: videoSettings.height!,
  },
  audio: {
    codec: 'aac',
    numberOfChannels: audioSettings.channelCount!,
    sampleRate: audioSettings.sampleRate!,
  },
  firstTimestampBehavior: 'offset',
  fastStart: 'in-memory',
});
```


Then we'll create a muxer writer for each track

``` typescript
createVideoMuxerWriter(muxer: Muxer<StreamTarget>): WritableStream<{ chunk: EncodedVideoChunk; meta: EncodedVideoChunkMetadata }> {
  return new WritableStream({
    async write(value) {
      muxer.addVideoChunk(value.chunk, value.meta);
    }
  });
}
function createAudioMuxerWriter(muxer: Muxer<StreamTarget>): WritableStream<EncodedAudioChunk> {
  return new WritableStream({
    async write(chunk) {
      muxer.addAudioChunk(chunk);
    }
  });
}
```

Then we can create a pipeline. 

![](/assets/patterns/livestreaming/mst-4.svg)


The AbortController allows us to stop the pipelines at any time via `abortController.abort()`, and so you could create a  'stop recording' button and have that call `abortController.abort()`  to stop recording.

```typescript
const abortController = new AbortController();

const videoPipeline  = videoProcessor.readable
.pipeThrough(videoEncoderStream)
.pipeTo(createVideoMuxerWriter(muxer), { signal: abortController.signal });

const audioPipeline = audioProcessor.readable
.pipeThrough(audioEncoderStream)
.pipeTo(createAudioMuxerWriter(muxer), { signal: abortController.signal })

```

We can await the pipelines, and the promise will resolve when the pipelines are done. We can then get our recorded video back;

```typescript
// Resolves when we call abortController.abort()
await Promise.all([videoPipeline, audioPipeline]); 
muxer.finalize();
const blob =  storage.toBlob('video/mp4');

```


Putting it all together, we can now record video in the browser without `MediaRecorder`, but rather by extracting `VideoFrame` and `AudioData` objects from the `MediaStream`, piping those through encoders, and streaming the results to a file.

<iframe src="/demo/webcam-recording/index.html" frameBorder="0" style="width: 720px; height: 320px; solid #e2e8f0; border-radius: 8px;"></iframe>


See the full code [here](https://github.com/sb2702/webcodecs-examples/blob/main/src/webcam-recording/recorder.ts)



## Streaming video over MoQ

Now that we can covered:

1. How to send data over a Media over Quic (MoQ) relay
2. How to grba a stram of encoded audio & video from a webcam

The next obvious step is to send encoded video over the network. We're going to start with sending data between browsers as it's the simplest case. 

The way to send arbitrary binary data over a track using the `@moq/lite` library is as follows:

```typescript
// Pull-mode, tracks are created when subscribers subscribe
const {track, priority} =  await broadcast.requested();

if(track.name ==='video'){
  const group = track.appendGroup();
  group.writeFrame(<Uint8Array> buffer); //whatever you want
  group.close();
}
```

Where you create a *group*, and in that *group* you can attach an arbitrary number of `Uint8Array` buffers. And as we saw, `EncodedVideoChunk` objects are just binary data + metadata.

MoQ doesn't provide a specific protocol for how to send video or audio data. A core aspect of MoQ is that it is content agnostic, so you can come up with any schema you want for transmitting encoded chunks, as long as it is packaged as a `Uint8Array`.

This goes well with WebCodecs, in that both technologies gives you much more control over encoding/data-transfer than is possible with WebRTC. 

If you just want something that works though, you can use an existing protocol like Hang which both works and is incredibly simple.

#### Hang Protocol

Hang is a protocol for transmitting streaming media via MoQ. There is a [library](https://github.com/moq-dev/moq/tree/main/js/hang) to implement this, but the protocol is simple enough to implement yourself.


###### Catalog.json
Hang works by the publisher publishing a track called `'catalog.json'` with the available video and audio tracks to consume with the following structure:

```json

{
  "video": {
    "renditions": {
       /*video0 is the name of the video track */
      "video0": {
        "codec": "avc1.64001f",
        "description": "0164001fffe100196764001fac2484014016ec0440000003004000000c23c60c9201000568ee32c8b0",
        "codedWidth": 1280,
        "codedHeight": 720
      }
    },
    "priority": 1
  },
  "audio": {
    "renditions": {
      /*audio1 is the name of the audio track */
      "audio1": {
        "codec": "mp4a.40.2",
        "sampleRate": 44100,
        "numberOfChannels": 2,
        "bitrate": 283637
      }
    },
    "priority": 2
  }
}

```

Where you specify available 'audio' tracks under 'audio' and video tracks under 'video'. Each available track is listed under `'renditions'`, and you'd specify the track name for each track.

The value for `catalog.video.tracks[trackName]` is the `VideoDecoderConfig` you need to decode the video track, and `catalog.audio.tracks[trackName]` has the config needed for `AudioDecoderConfig`.


So to actually connect to a session, and be ready to start sending data, we'd have the publisher publish a broadcast, and have each subscriber subscribe to a broadcast, which provides the namespace.

<Tabs>


  <TabItem label="Publisher">


  ```typescript
import * as Moq from "@moq/lite";
const connection = await Moq.connect("https://relay.moq.some-cdn.com");
const broadcast = new Moq.Broadcast();
connection.publish('my-broadcast', broadcast);
```
  
  </TabItem>

  <TabItem label="Subscriber">

```typescript
import * as Moq from "@moq/lite";
const connection = await Moq.connect("https://relay.moq.some-cdn.com");
const broadcast = connection.consume("my-broadcast");
```
  </TabItem>
</Tabs>

Then, the publisher will publish track info via catalog.json. Because MoQ lite is built in a pull manner, the publisher listens for `catalog.json` track requests.

``` typescript
for (;;) {
    const trackRequest = await broadcast.requested();
    const requestedTrack = trackRequest.track;
    if (requestedTrack.name === 'catalog.json') {
      const catalogJson = JSON.stringify(catalogData);
      const group = requestedTrack.appendGroup();
      group.writeString(catalogJson);
      group.close();
    }
}
```
Any subscriber can then subscribe to the  `catalog.json` track in the broadcast, and once it receives the catalog, it can start listening for the video and audio tracks, as well as set up the `AudioDecoder` and `VideoDecoder`

```typescript
const catalogTrack = broadcast.subscribe('catalog.json');

for (;;) {
  const catalogGroup = await catalogTrack.nextGroup();
  if (catalogGroup) {
    const catalogJson = await catalogGroup.readString();
    const catalog = JSON.parse(catalogJson);
    // You know what tracks to listen for
  }
}
```


###### Streaming chunks

The catalog has most of the data now needed to decode video or audio chunks, but there's still two key bits of info needed: which chunks are key frames, and the timestamp of each chunk.

First, to encode the timestamp, we'll actally use a fixed 8-byte header to store the timestamp as an unsigned 8-byte integer.

```typescript
//EncodedVideoChunk or EncodedAudioChunk
const timestamp = chunk.timestamp; 
const header = new Uint8Array(8);
const view = new DataView(header.buffer, header.byteOffset, 8);
view.setBigUint64(0, BigInt(timestamp));
```
Then, to encode the chunk, we'll create a `Uint8Array` which is the size of the encoded chunk + the header byteLength, and then we set the header at position 0, and the chunk data at position 8 (header byteLength).

```typescript
const data = new Uint8Array(header.byteLength + chunk.byteLength);
data.set(header, 0);
chunk.copyTo(data.subarray(header.byteLength));  
```

The receiver would parse the header to get the timestamp, and rest of the array would be the actual chunk data. 

This just leaves the question of which chunks are keyFrames. The way we can handle this is by writing chunks in groups

```typescript

let group: Moq.Group | undefined;

const encoder = new VideoEncoder({
  output: (frame: EncodedVideoChunk) => {
    if (frame.type === "key") {
      groupTimestamp = frame.timestamp as Time.Micro;
      group?.close();
      group = track.appendGroup();
    } 
    const buffer = encodeFrame(frame); // Logic from above
    group?.writeFrame(buffer);
  },
  error: (err: Error) => {
    track.close(err);
    group?.close(err);
  },
});

```

Then, when the subscriber listens for new groups in the track, the first frame in the group is always a key frame

```typescript

const track = await broadcast.subscribe("videoTrack");

// Read data as it arrives
for (;;) {
	const group = await track.nextGroup();
	if (!group) break;
  let keyframe = true; // First is always a key frame
	for (;;) { 
		const frame = await group.readFrame(); 
    if(frame){
        parseFrame(frame, keyframe);
        keyframe = false;
    }
  }
}
```

While MoQ was built to be content agnostic, one reason for including `groups` as a core aspect was to enable sending groups of pictures (a key frame and all it's subsequent delta frames), to improve streaming stability

#### Browser to Browser Streaming

With the Hang protocol, we now have everything to start streaming WebCodecs audio and video  over MoQ. We'll start by defining a publisher, which takes in the Audio/Video Config, listens for audio/video tracks, and creates audio/video pipelines to stream over MoQ.

<details>

<summary>Publisher </summary>

```typescript
import { MediaStreamTrackProcessor, getSampleRate } from 'webcodecs-utils';
import { VideoEncoderStream } from './video-encoder-stream'; // TransformStream discussed above
import { AudioEncoderStream } from './audio-encoder-stream'; // TransformStream discussed above

export class MoqPublisher {
  private videoTrack: MediaStreamTrack;
  private audioTrack: MediaStreamTrack;
  private broadcast: any;
  private videoConfig: VideoEncoderConfig;
  private audioConfig: AudioEncoderConfig;
  private videoMoqTrack: any = null;
  private audioMoqTrack: any = null;
  private abortController: AbortController | null = null;

  constructor(
    videoTrack: MediaStreamTrack,
    audioTrack: MediaStreamTrack,
    broadcast: any,
    videoConfig: VideoEncoderConfig,
    audioConfig: AudioEncoderConfig
  ) {
    this.videoTrack = videoTrack;
    this.audioTrack = audioTrack;
    this.broadcast = broadcast;
    this.videoConfig = videoConfig;
    this.audioConfig = audioConfig;
  }

  async start(): Promise<void> {
    if (this.abortController) {
      throw new Error('Already publishing');
    }

    this.abortController = new AbortController();
    
    for(;;){ // Listen for track requests
      const trackRequest = this.broadcast.requested();
      if(trackRequest) this.handleTrackRequest(trackRequest)
      await new Promise((r)=>requestAnimationFrame(r));
    }

  
  }

  async handleTrackRequest(trackRequestPromise){

    const trackRequest = await trackRequestPromise;
    const requestedTrack = trackRequest.track;

    if (requestedTrack.name === 'video' && !this.videoMoqTrack) {

      this.videoMoqTrack = requestedTrack;

      // Video pipeline
      const videoProcessor = new MediaStreamTrackProcessor({ track: this.videoTrack });
      const videoEncoderStream = new VideoEncoderStream(this.videoConfig);

      // Start video pipeline
      videoProcessor.readable
        .pipeThrough(videoEncoderStream)
        .pipeTo(this.createVideoWriter(this.videoMoqTrack), {
          signal: this.abortController.signal
        });
    } else if (requestedTrack.name === 'audio' && !this.audioMoqTrack) {
      this.audioMoqTrack = requestedTrack;

          // Audio pipeline
    const audioProcessor = new MediaStreamTrackProcessor({ track: this.audioTrack });
    const audioEncoderStream = new AudioEncoderStream(this.audioConfig);

      // Start audio pipeline
      audioProcessor.readable
        .pipeThrough(audioEncoderStream)
        .pipeTo(this.createAudioWriter(this.audioMoqTrack), {
          signal: this.abortController.signal
        });
    }

  }

  private createVideoWriter(moqTrack: any): WritableStream<{ chunk: EncodedVideoChunk; meta: EncodedVideoChunkMetadata }> {
    let currentGroup: any = null;

    return new WritableStream({
      async write(value: EncodedVideoChunk) {
        // Start new group on keyframe (GOP - group of pictures)
        if (value.chunk.type === 'key') {
          if (currentGroup) {
            currentGroup.close();
          }
          currentGroup = moqTrack.appendGroup();
        }

        if (!currentGroup) {
          // First chunk must be a keyframe
          currentGroup = moqTrack.appendGroup();
        }

        // Hang format: [timestamp (8 bytes)] [data]
        const chunkData = new Uint8Array(value.chunk.byteLength);
        value.chunk.copyTo(chunkData);

        const buffer = new Uint8Array(8 + chunkData.byteLength);
        const view = new DataView(buffer.buffer);

        // Write timestamp as 64-bit integer (microseconds)
        view.setBigUint64(0, BigInt(value.chunk.timestamp), true);

        // Write chunk data
        buffer.set(chunkData, 8);

        currentGroup.writeFrame(buffer);
      },
      async close() {
        if (currentGroup) {
          currentGroup.close();
        }
      }
    });
  }

  private createAudioWriter(moqTrack: any): WritableStream<EncodedAudioChunk> {
    return new WritableStream({
      async write(chunk: EncodedAudioChunk) {
        const group = moqTrack.appendGroup();

        // Hang format: [timestamp (8 bytes)] [data]
        const chunkData = new Uint8Array(chunk.byteLength);
        chunk.copyTo(chunkData);

        const buffer = new Uint8Array(8 + chunkData.byteLength);
        const view = new DataView(buffer.buffer);

        // Write timestamp as 64-bit integer (microseconds)
        view.setBigUint64(0, BigInt(chunk.timestamp), true);

        // Write chunk data
        buffer.set(chunkData, 8);

        group.writeFrame(buffer);
        group.close();
      }
    });
  }

  stop(): void {
    if (this.abortController) {
      this.abortController.abort();
      this.abortController = null;
    }
  }
}

```
</details>

Next we define the subscriber

<details>

<summary> Subscriber </summary>


```typescript
export interface MoqFrame {
  timestamp: number;
  type?: 'key' | 'delta';
  data: Uint8Array;
}

export class MoqSubscriber {
  private videoTrack: any;
  private audioTrack: any;
  private videoDecoder: VideoDecoder | null = null;
  private audioDecoder: AudioDecoder | null = null;
  private catalog: any;

  constructor(catalog: any, videoTrack: any, audioTrack: any) {
    this.catalog = catalog;
    this.videoTrack = videoTrack;
    this.audioTrack = audioTrack;
  }

  async startVideo(onFrame: (frame: VideoFrame) => void): Promise<void> {
    // Get video config from catalog
    const videoRendition = Object.values(this.catalog.video.renditions)[0] as any;

    this.videoDecoder = new VideoDecoder({
      output: onFrame,
      error: (e) => console.error('Video decoder error:', e),
    });

    const config: VideoDecoderConfig = {
      codec: videoRendition.codec,
      codedWidth: videoRendition.codedWidth,
      codedHeight: videoRendition.codedHeight,
    };

    this.videoDecoder.configure(config);

    // Start reading video frames
    (async () => {
      try {
        while (true) {
          const group = await this.videoTrack.nextGroup();
          if (!group) break;

          // First frame in group is always a keyframe
          let isKeyframe = true;

          // Read all frames in the group
          for (;;) {
            const frameData = await group.readFrame();
            if (!frameData) break;

            const frame = this.parseVideoFrame(frameData, isKeyframe);

            const chunk = new EncodedVideoChunk({
              timestamp: frame.timestamp,
              type: frame.type!,
              data: frame.data,
            });

            this.videoDecoder!.decode(chunk);
            isKeyframe = false; // Subsequent frames are delta
          }
        }
      } catch (error) {
        console.error('Video read error:', error);
      }
    })();
  }

  async startAudio(onData: (audioData: AudioData) => void): Promise<void> {
    // Get audio config from catalog
    const audioRendition = Object.values(this.catalog.audio.renditions)[0] as any;

    this.audioDecoder = new AudioDecoder({
      output: onData,
      error: (e) => console.error('Audio decoder error:', e),
    });

    this.audioDecoder.configure({
      codec: audioRendition.codec,
      sampleRate: audioRendition.sampleRate,
      numberOfChannels: audioRendition.numberOfChannels,
    });

    // Start reading audio frames
    (async () => {
      try {
        while (true) {

          const group = await this.audioTrack.nextGroup();
          if (!group) break;

          const frameData = await group.readFrame();
          const frame = this.parseAudioFrame(frameData);

          const chunk = new EncodedAudioChunk({
            timestamp: frame.timestamp,
            type: 'key',
            data: frame.data,
          });

          this.audioDecoder!.decode(chunk);
        }
      } catch (error) {
        console.error('Audio read error:', error);
      }
    })();
  }

  private parseVideoFrame(buffer: Uint8Array, isKeyframe: boolean): MoqFrame {
    // Hang format: [timestamp (8 bytes)] [data]
    const view = new DataView(buffer.buffer, buffer.byteOffset);

    const timestamp = Number(view.getBigUint64(0, true));
    const type = isKeyframe ? 'key' : 'delta';
    const data = buffer.slice(8);

    return { timestamp, type, data };
  }

  private parseAudioFrame(buffer: Uint8Array): MoqFrame {
    // Hang format: [timestamp (8 bytes)] [data]
    const view = new DataView(buffer.buffer, buffer.byteOffset);

    const timestamp = Number(view.getBigUint64(0, true));
    const data = buffer.slice(8);

    return { timestamp, data };
  }

  stop(): void {
    if (this.videoDecoder && this.videoDecoder.state !== 'closed') {
      this.videoDecoder.close();
    }
    if (this.audioDecoder && this.audioDecoder.state !== 'closed') {
      this.audioDecoder.close();
    }
  }
}

```

</details>




We still need the audio player

<details> 
<summary>Audio Player </summary>


```typescript
export class AudioPlayer {
  private audioContext: AudioContext;
  private gainNode: GainNode;
  private sampleRate: number;
  private numberOfChannels: number;
  private nextPlayTime: number = 0;

  constructor(sampleRate: number, numberOfChannels: number) {
    this.sampleRate = sampleRate;
    this.numberOfChannels = numberOfChannels;
    this.audioContext = new AudioContext({ sampleRate });
    this.gainNode = this.audioContext.createGain();
    this.gainNode.connect(this.audioContext.destination);
  }

  play(audioData: AudioData): void {
    // Extract PCM data from AudioData
    const numberOfFrames = audioData.numberOfFrames;
    const buffer = this.audioContext.createBuffer(
      this.numberOfChannels,
      numberOfFrames,
      this.sampleRate
    );

    // Copy data for each channel
    for (let channel = 0; channel < this.numberOfChannels; channel++) {
      const channelData = new Float32Array(numberOfFrames);
      audioData.copyTo(channelData, {
        planeIndex: channel,
        format: 'f32-planar',
      });
      buffer.copyToChannel(channelData, channel);
    }

    // Create and schedule buffer source
    const source = this.audioContext.createBufferSource();
    source.buffer = buffer;
    source.connect(this.gainNode);

    const currentTime = this.audioContext.currentTime;
    const playTime = Math.max(currentTime, this.nextPlayTime);

    source.start(playTime);
    this.nextPlayTime = playTime + buffer.duration;
  }

  setVolume(volume: number): void {
    this.gainNode.gain.value = volume;
  }

  close(): void {
    this.audioContext.close();
  }
}

```

</details>


Now finally we can add the interface which will load the publisher and subscriber respectively

<details>

<summary> Interface </summary>



<Tabs>

  <TabItem label="Publisher">


  ```html
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>MoQ Publisher</title>
</head>
<body>
  <div class="container">
    <h3>ðŸ“¤ Publisher</h3>
    <div id="status" class="status">Initializing...</div>

    <div class="controls">
      <button id="startWebcamBtn">Start Webcam</button>
      <button id="startPublishBtn" disabled>Start Publishing</button>
    </div>

    <video id="preview" autoplay muted playsinline style="width: 100%; max-width: 355px; border-radius: 4px; margin: 10px 0;"></video>

    <div class="log" id="log"></div>
  </div>



  <script type="module">
    import * as Moq from 'https://esm.sh/@moq/lite';
    import { MoqPublisher } from '../../../src/moq/moq-publisher.ts';
    import { getSampleRate, getBitrate, getCodecString } from 'webcodecs-utils';
    
    // Get URL parameters
    const params = new URLSearchParams(window.location.search);
    const relayUrl = params.get('relay') || 'https://relay.quic.video:4443';
    const broadcastName = params.get('broadcast') || 'my-broadcast';

    // UI Elements
    const startWebcamBtn = document.getElementById('startWebcamBtn');
    const startPublishBtn = document.getElementById('startPublishBtn');
    const preview = document.getElementById('preview');

    // State
    let connection = null;
    let broadcast = null;
    let videoTrack = null;
    let audioTrack = null;
    let stream = null;
    let publisher = null;


    async function startWebcam() {
      try {

        stream = await navigator.mediaDevices.getUserMedia({
          video: { width: 1280, height: 720 },
          audio: true,
        });

        videoTrack = stream.getVideoTracks()[0];
        audioTrack = stream.getAudioTracks()[0];

        preview.srcObject = stream;

        startWebcamBtn.disabled = true;
        startPublishBtn.disabled = false;
      } catch (error) {
 
        console.log('Webcam error: ' + error.message, 'error');
      }
    }

    async function startPublishing() {
      try {

        connection = await Moq.Connection.connect(new URL(relayUrl));
        broadcast = new Moq.Broadcast();
        connection.publish(broadcastName, broadcast);


        startPublishBtn.disabled = true;

        // Create encoder configs
        const videoSettings = videoTrack.getSettings();
        const audioSettings = audioTrack.getSettings();
        const sampleRate = await getSampleRate(audioTrack);

        const codec = 'avc';
        const bitrate = getBitrate(videoSettings.width, videoSettings.height, 30, 'good');
        const codecString = getCodecString(codec, videoSettings.width, videoSettings.height, bitrate);

        const videoConfig = {
          codec: codecString,
          width: videoSettings.width,
          height: videoSettings.height,
          bitrate: Math.round(bitrate),
          framerate: 30,
        };

        const audioConfig = {
          codec: 'opus',
          sampleRate: sampleRate,
          numberOfChannels: Math.min(audioSettings.channelCount || 2, 2),
          bitrate: 128000,
        };

        // Get description using the same config
        const description = await MoqPublisher.getDescription(videoTrack, videoConfig);

        const catalogData = {
          video: {
            renditions: {
              video0: {
                codec: codecString,
                codedWidth: videoSettings.width,
                codedHeight: videoSettings.height,
                description
              }
            },
            priority: 1
          },
          audio: {
            renditions: {
              audio0: audioConfig
            },
            priority: 2
          }
        };

        // Handle catalog requests
        (async () => {
          while (true) {
            const trackRequest = await broadcast.requested();
            const requestedTrack = trackRequest.track;

            if (requestedTrack.name === 'catalog.json') {
              const catalogJson = JSON.stringify(catalogData);
              const group = requestedTrack.appendGroup();
              group.writeString(catalogJson);
              group.close();
            }
          }
        })();

        // Start MoQ publisher with configs
        publisher = new MoqPublisher(videoTrack, audioTrack, broadcast, videoConfig, audioConfig);
        await publisher.start();
        log('Publisher started', 'success');

      } catch (error) {
        console.log('Error: ' + error.message, 'error');
      }
    }
    // Event listeners
    startWebcamBtn.addEventListener('click', startWebcam);
    startPublishBtn.addEventListener('click', startPublishing);
  </script>
</body>
</html>


```
  
  </TabItem>

  <TabItem label="Subscriber">
```html
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>MoQ Subscriber</title>
</head>
<body>
  <div class="container">
    <h3>ðŸ“¥ Subscriber</h3>
    <div id="status" class="status">Initializing...</div>

    <canvas id="canvas" width="640" height="360" style="width: 100%; max-width: 355px; border-radius: 4px; margin: 10px 0; background: #000;"></canvas>

    <div class="log" id="log"></div>
  </div>

  <script type="module">
    import * as Moq from 'https://esm.sh/@moq/lite';
    import { MoqSubscriber, AudioPlayer } from '../../../src/moq/index.ts';

    // Get URL parameters
    const params = new URLSearchParams(window.location.search);
    const relayUrl = params.get('relay') || 'https://relay.quic.video:4443';
    const broadcastName = params.get('broadcast') || 'my-broadcast';

    // UI Elements
    const statusEl = document.getElementById('status');
    const logEl = document.getElementById('log');
    const canvas = document.getElementById('canvas');
    const ctx = canvas.getContext('2d');

    // State
    let connection = null;
    let subscriber = null;
    let frameCount = 0;


    async function getCatalog(broadcast) {
      try {
        const catalogTrack = broadcast.subscribe('catalog.json');

        for (;;) {
          const catalogGroup = await catalogTrack.nextGroup();

          if (catalogGroup) {
            const catalogJson = await catalogGroup.readString();
            const catalog = JSON.parse(catalogJson);
            log('Received catalog', 'success');
            console.log('Catalog:', catalog);
            return catalog;
          }
        }
      } catch (e) {
        await new Promise((r) => setTimeout(r, 500));
        return await getCatalog(broadcast);
      }
    }




    async function init() {
      try {
      
        connection = await Moq.Connection.connect(new URL(relayUrl));

        const broadcast = connection.consume(broadcastName);

        const catalog = await getCatalog(broadcast);


        const videoTrack = await broadcast.subscribe('video');

       const audioTrack = await broadcast.subscribe('audio');

 

        // Create audio player
        const audioRendition = Object.values(catalog.audio.renditions)[0];
        const audioPlayer = new AudioPlayer(audioRendition.sampleRate, audioRendition.numberOfChannels);

        subscriber = new MoqSubscriber(catalog, videoTrack, audioTrack);

        // Start video decoding and rendering
        subscriber.startVideo((frame) => {
          ctx.drawImage(frame, 0, 0, canvas.width, canvas.height);
          frame.close();
          frameCount++;
        });

        // Start audio decoding and playback
        subscriber.startAudio((audioData) => {
          audioPlayer.play(audioData);
          audioData.close();
        });

        updateStatus('Receiving stream', 'success');
        log('Decoders started', 'success');

      } catch (error) {
        updateStatus('Connection failed', 'error');
        log('Init error: ' + error.message, 'error');
      }
    }

    // Initialize
    init();
  </script>
</body>
</html>

```
  </TabItem>
</Tabs>

</details>

We can now finally put everything together in a live demo, where it will stream your webcam through Media over Quic (via a public cloudflare relay) from the publisher tab to the subscriber tab.

<iframe src="/demo/moq/webcam/index.html" style="width: 100%; height: 700px; border: 1px solid #e2e8f0; border-radius: 8px;"></iframe>


And that's it! That's the basics of streaming audio and video over Media over Quic. You can find the source code for the demo here: ([HTML](https://github.com/sb2702/webcodecs-examples/tree/main/demos/moq/browser-to-browser), [JS](https://github.com/sb2702/webcodecs-examples/tree/main/src/moq)).


### Browser to Server

-- parsing the data locally

-- Saving it locally

-- Production gotchas



### Server to browser

-- Creating a live stream







## Alternatives

#### WebRTC

#### Media Source Extensions


## Case Studies

Now that you've seen how WebCodecs works and how to stream, let's look at some concrete examples of actual streaming applications and their architectures: [Session](https://www.youtube.com/watch?v=WispwTzPS9A) (a webinar tool, discontinued), and [Streamyard](https://streamyard.com/) (a browser based recording studio). I worked on both products in a previous role.

#### Session (Webinars)

Session was a webinar product which enabled several hosts to conduct video webinars with up to 5000 participants. It had common interactive webinar features like polls, Q&A and breakout rooms.

![](/assets/patterns/livestreaming/session2.jpeg)

Session was built with [WebRTC](https://webrtc.org/), which is an WebAPI that is specifically designed to facilitate video conferencing, and is what is used in browser based video conference tools like Google Meet.

Session used a router/relay model, in which every participant streamed audio/video a routing server, and then a server (specifically an [SFU](https://bloggeek.me/webrtcglossary/sfu/)) would then apply it's business logic to route some subset of streams to each participant without re-encoding.

![](/assets/patterns/livestreaming/session-stack.svg)

In the webinar use case, the hosts would broadcast to all participants (few to many), but each participant would only see a subset of other participants, and this subset could change based on factors like joining a breakout room, and participants joining and leaving.

WebRTC was probably the best solution in this case, as it facilitates coordination of codec choice among all participants, abstracts the encoding details and is optimized for real-time delivery. However, there were still some challenges with using WebRTC for Session:

**Scale**: WebRTC starts facing scalability issues beyond 100 participants [[1](https://bloggeek.me/how-many-users-webrtc-call/)]. This was fine for most webinars, but a few high-value webinars attracted thousands of participants, and Session was only able to scale to 5000 participants through in-house WebRTC expertise.

**Video Quality**: WebRTC provided limited control over video quality, and by necessity provided lower quality to ensure low latency. This became relevant when enabling recording of webinars (a common use case), where the recording was essentially a server joining the webinar as a participant, and recording the compressed video feeds in real time, resulting in *okay* quality recordings, but not matching the level of locally recorded content.

WebCodecs via Media over Quic could likely have solved the scale issue, handling many more concurrent subscribers than is possible with WebRTC. Media over Quic is particularly well suited to this "middle-ground" for webinars where there are too many participants for a normal video conferencing call (for which WebRTC was designed), but not enough scale for full-fledged HLS/DASH streaming with millions of viewers.


However as a newer protocol with fewer supporting libraries, and more decision making required on codec configuration and delivery, it would have required more up-front engineering and experimentation to work. 


#### Streamyard

Streamyard is a browser based recording studio, that enables participants to live stream their webcam feeds to multiple social media destinations (like Facebook Live, YouTube live) simultaneously.

![](/assets/patterns/livestreaming/streamyard.png)

Streamyard had a more complicated setup where participants streamed their webcam feeds via WebRTC through a routing server. Functionally, it operated like a WebRTC conferencing call, but there were also recording servers listening to each "call" as another WebRTC participant (running a version of the app on the server via headless browsers), recording the feeds from each participant and compositing them into the layout the host specified in the studio.

![](/assets/patterns/livestreaming/streamyard-stack.svg)

This composited feed would then be encoded into an RTMP live stream that would then be simultaneously distributed to multiple destinations (e.g. Facebook Like, YouTube live etc...). Streamyard was built before WebCodecs was released, so it wasn't an option when the product first launched.

**Video Quality**: Many live streamers wanted higher quality versions of their recordings which they could edit after the live stream. Because the WebRTC feeds provided lower video quality (to ensure latency), Streamyard implemented a system to record higher-quality versions of each stream within the browser (called "local crecordings"), and send that to the server in chunks, in parallel to the WebRTC stream. This meant that in practice, the browser was recording two video streams in parallel.


**Complexity**: Because feeds were being converted to RTMP feeds and sent to livestreaming destinations with their own CDNs, Streamyard never faced a scale issue, however the mixing of numerous technologies added complexity and failure points, and it worked at a business level only because of lots of engineering and a deep internal focus on stability.


While working with RTMP was a core requirement (that's what live-streaming destinations required as inputs), Streamyard had multiple options for sending video from the browser to the server, and in practice ended up using two parallel systems (WebRTC for the livestream, an in-house solution for local recordings).

If you were to build a product like Streamyard today, given the product's core focus on stability, using the established WebRTC protocol over a newer, more experimental setup with WebCodecs probably makes sense. That said, I do know that the Streamyard team specifically preferred controlling certain aspects of their stack to obtain more reliability and performance compared to off-the-shelf solutions, and so having more fine-grained control over encoding and networking compared to WebRTC might would have also made it a sensible choice.

For the local recordings use case (recording higher-quality versions of the stream locally, uploading that in chunks), that seems like a pretty good use case for WebCodecs.


#### Depends on your use case

Hopefully you can see from the above-two examples that how you architect a streaming application depends on what you are doing, you may have multiple options for streaming media, each option has it's strengths and weaknesses, and business requirements would often require custom logic beyond what any standard technology can provide by itself.

Maybe WebCodecs doesn't make sense for your application, or maybe it's a no brainer, or maybe it might make sense for certain features.  Whatever the case, hopefully this article gave you an idea of how WebCodecs can be used in streaming contexts, and provided enough info to make a more informed decision on how to architect a streaming application.